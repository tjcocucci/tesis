\chapter{Introducción}
En esta tesis exploramos algunos de los desafíos asociados a la aplicación de técnicas de asimilación de datos basadas en ensambles sobre modelos epidemiológicos. Abordamos el problema de la especificación de la incerteza inherente al modelo y las observaciones tanto en un marco general de modelos parcialmente observados como para el caso específico de modelos epidemiológicos basados en ecuaciones diferenciales. Además estudiamos el potencial de utilizar técnicas de asimilación de datos en modelos basados en agentes.

Muchos sistemas complejos suelen ser de alta dimensionalidad. Por ejemplo, sistemas sociales, geofísicos, ecológicos, etc. suelen tener esta característica. Además, debido a su tamaño y complejidad suele ser difícil tomar mediciones de su estado. La información que se consigue a través de observaciones es entonces parcial y con errores de significativos. En este contexto se requiere incluir información basada en el conocimiento del sistema. Para esto, se pueden considerar modelos matemáticos con el fin de generar pronósticos y predecir su comportamiento. Estas dos fuentes de información están en principio incomunicadas. La asimilación de datos comprende un conjunto de técnicas estadísticas que se utilizan para combinar estas fuentes de información sobre el estado del sistema: pronósticos provenientes de modelos matemáticos y observaciones \citep{Kalnay2003}. Ambas fuentes de información son propensas a errores. El error de modelo cuantifica nuestro conocimiento limitado de la dinámica del sistema, aproximaciones y errores numéricos. El error observacional incluye la incerteza propia de los instrumentos de medición y el error de representatividad que involucra como se relacionan las observaciones con el estado del sistema. Esto se describe con mayor detalle en la Sección \ref{sec:model_obs_error}. La asimilación de datos apunta a encontrar una combinación ponderada entre estas fuentes de información, de manera que si sabemos que la incerteza del modelo es menor que la de los datos, la estimación resultante será más fiel al modelo y si por el contrario, las observaciones tienen menos error que el pronóstico la estimación será más próxima a los datos.

Una de las aplicaciones pioneras de la asimilacion de datos es la predicción numérica meteorológica. La meteorología es un sistema complejo por antonomasia, con numerosos procesos dinámicos de diferentes escalas que interactuan entre sí. Por las características de la atmósfera y la tierra las observaciones son escasas. Esto motivó originalmente los desarrollos de la asimilación variacional de datos \citep{Talagrand1987}. Este enfoque permite reformular el problema como la minimización de una función de costo que penaliza a las fuentes de información de mayor incerteza. En el área de la predicción meteorológica numérica se cuenta con modelos matemáticos y computacionales muy complejos y de alta dimensionalidad que describen la evolución de los procesos físicos e informan sobre diversas variables de estado (por ejemplo, velocidad, temperatura o presión) en diferentes puntos de una grilla espacial potencialmente muy grande ($10^7 - 10^8$ dimensiones). Estos modelos se basan en leyes físicas y permiten obtener pronósticos: por ejemplo, las ecuaciones de Navier-Stokes que expresan la conservación del momento y de la masa en fluidos. Por otro lado, se tiene otra fuente de información sobre el mismo sistema que consta de las observaciones de diversos instrumentos en estaciones meteorológicas o provenientes de satélites. 

El filtro de Kalman \citep{Kalman1960, Kalman1961} ocupa un lugar central dentro de las técnicas de asimilación de datos pues es una metodología sencilla que ha sentado las bases para métodos más sofisticados. Este tipo de filtro lineal encontró aplicaciones relevantes en la determinación de órbitas satelitales, navegación de submarinos y aeronaves e incluso de misiones espaciales como la Apollo \citep{Jazwinski1970}. En esta clase de aplicaciones típicamente tenemos que, con el fin de estimar la posición y velocidad, se utiliza como modelo a las ecuaciones físicas de movimiento mientras que las observaciones provienen de los instrumentos de navegación. Una gran parte del desarrollo de técnicas de asimilación de datos proviene sin embargo del área de las geociencias donde se presentan otro tipo de desafíos como la alta dimensionalidad de los sistemas, observaciones menos precisas y modelos caóticos altamente no lineales. La alta dimensionalidad de estos sistemas hace imposible la representación de la matriz de covarianza de la prediccion requerida por el filtro de Kalman (de dimensiones inmensas, $10^7 \times 10^7$ o mayores). Esto motivó el desarrollo del filtro de Kalman por ensambles \citep{Evensen1994}, el cual toma la idea original de Kalman incorpora la idea de representar distribuciones mediante muestras lo cual permite adaptar el problema a situaciones de no linealidad. Así además se evita la representación explícita de la matriz de covarianza de las predicciones lo que permite la aplicación a sistemas de mayores dimensiones. La familia de métodos por ensambles pudo competir con los métodos variacionales (3D-VAR y 4D-VAR) que son utilizados en grandes centros meteorológicos \citep{Kalnay2007}.

Más allá que las técnicas de asimilación de datos fueron desarrolladas inicialmente para resolver problemas asociados a la predicción numérica meteorológica y para la aeronavegación espacial \citep{Grewal2010}, actualmente el campo de aplicación es mucho más amplio. Se utilizan, por ejemplo, para la predicción sobre reservorios petrolíferos \cite{Aanonsen2009}, oceanografía, detección de incendios forestales \citep{Mandel2008}, epidemiología \citep{Shaman2012}, entre otras.

Desde el punto de vista conceptual, existen dentro de los métodos paramétricos no lineales de la asimilación de datos principalmente dos formulaciones: los filtros de Kalman por ensambles y los métodos variacionales. Por otro lado tenemos que, más recientemente, los filtros de partículas han cobrado relevancia. Estos están basados en la aplicación secuencial de Monte Carlo y habilitan la representación de distribuciones no Gaussianas. El conjunto conforma una gran familia de metodologías de creciente interés \citep{vanLeeuwen2019}. En general, los filtros de partículas no hacen suposiciones acerca de las distribuciones involucradas, por lo que en principio se pueden aplicar a cualquier variante del problema de asimilación de datos. Sin embargo, la representación de distribuciones totalmente generales mediante muestras requiere de un número muy grande de partículas lo que puede resultar privativo.

La variedad de metodologías de asimilación es vasta, en especial porque para cada técnica se tiene una plétora de variantes e implementaciones que se utlizan para mitigar los problemas que presenta cada aplicación. Así tenemos una familia de filtros de Kalman por ensambles, otra de métodos variacionales, otra de filtros de partículas, etc. Incluso, dentro de cada una de estas familias tenemos subgrupos de técnicas que comparten ciertas características o abordajes conceptuales. Más aún, nuevos enfoques apuntan a recuperar lo mejor de cada metodología mediante híbridos: por ejemplo existen híbridos entre filtros de Kalman por ensambles y métodos variacionales \citep{Hamill2000} o con filtros de partículas \citep{Stordal2011, Frei2013}. En \cite{Carrassi2018} se puede encontrar un buen panorama del estado del arte en cuanto a metodologías de asimilación de datos.

Actualmente el creciente interés en \textit{machine learning} y los grandes avances en el área en las últimas décadas motivó interés en el intercambio entre esta área y la asimilación de datos. En \cite{Abarbanel2018} se plantea una interpretación de las redes neuronales en el contexto de problemas de asimilación de datos dando una equivalencia entre agregar capas a la red neuronal con la resolución temporal en el problema de asimilación. En la asimilacion de datos variacional, el modelo adjunto es equivalente al método de \textit{backpropagation} y en ambos casos se determina la sensibilidad de la función de costo a párametros de la red o variables del sistema. Además, se señala la utilidad de algunas herramientas de asimilación para su aplicación en \textit{machine learning}, en particular el uso de \textit{variational annealing} para encontrar mínimos globales en funciones de costo. Por otro lado, en \cite{Kovachki2019} se propone el uso técnicas basadas en filtros de Kalman por ensambles para asistir el entrenamiento de modelos supervisados y semi-supervisados de manera que se evita el uso de gradientes. Potencialmente, esta metodología se podría utilizar para capturar la incerteza en una red neuronal entrenada usando la variabilidad del ensamble. Con estas estrategias se pueden entrenar y determinar los parametros de la red sin el requerimiento de \textit{backpropagation}; sólo con la evolución hacia adelante de un ensamble de estados del modelo se determinan las correlaciones entre las variables y los parámetros.

Como mencionamos anteriormente, la asimilación de datos tiene en cuenta la incerteza del modelo que genera los pronósticos del sistema tanto como la proveniente de las observaciones. Una especificación errónea de estas cantidades puede causar una performance subóptima de la inferencia pero es habitual que estos errores sean difíciles de identificar. En general, en la práctica, se les da valores \textit{a priori} a los errores asociados al modelo y las observaciones pero esto puede ser en detrimento del desempeño del sistema. Este problema constituye uno de los mayores desafíos en el área y se han desarrollado una variedad de métodos para proveer estimaciones para el error observacional y de modelo \citep{Tandeo2020}. Estos incluyen metodologías basadas en momentos estadísticos y otras que apuntan a la maximización de la verosimilitud. Entre estas últimas hacemos mención del algoritmo EM \citep{Dempster1977}, el cual puede ser implementado en el contexto de asimilación de datos con filtros de Kalman y en particular con filtros basados en ensambles \citep{Tandeo2015}.

La implementación clásica del algoritmo EM toma un lote (o \textit{batch}) de datos y los procesa a todos juntos para dar proveer estimaciones de los parámetros que codifican los errores. Esto trae una serie de inconvenientes a la hora de ser utilizado en sistemas de asimilación secuencial en los que los datos son producidos y procesados en tiempo real o casi real. El procesamiento por lotes es \textit{offline} por lo que, por un lado, fuerza a almacenar todo el conjunto de observaciones y por otro, cuando ingresa una observación no es en principio adaptable a incorporarla por lo que el proceso debe reiniciarse. Esta situación ha despertado interés en técnicas de inferencia \textit{online}, es decir que permitan actualizaciones con cada nueva observación y de manera que cada dato es procesado una única vez. En esta dirección se destaca la metodología desarrollada en \cite{Berry2013} la cual es basada en innovaciones (esto, a grandes rasgos, significa que se basa en las diferencias entre pronósticos y observaciones) y que está específicamente diseñada para contextos de asimilación de datos. Por otro lado, luego del trabajo seminal de \cite{Neal1998} tenemos versiones \textit{online} del EM por \cite{Andrieu2003} y por Cappé \citep{Cappe2009,Cappe2011}. Estas sin embargo no están explícitamente diseñadas para ser acopladas a técnicas clásicas de asimilación de datos. Por el interés en metodologías de este tipo y su relevancia, en esta tesis planteamos el desarrollo de versiones \textit{online} del EM que se adapta a métodos de asimilación por ensambles, tanto filtros de partículas como el filtro de Kalman por ensambles. Además evaluamos experimentalmente la metodología en escenarios típicos de asimilación de datos.

El marco en el que se suele aplicar la asimilación de datos es el de un sistema que evoluciona temporalmente y es parcialmente observado. Estos interpretarse teóricamente como modelos de Markov escondidos o \textit{state-space models}. Los sistemas epidemiológicos tienen estas características y por lo tanto la utilización de herramientas la asimilación de datos en estos sistemas puede ser provechosa. Desde comienzos del siglo pasado se han utilizado modelos matemáticos para estudiar las dinámicas de transmisión de enfermedades. Los populares modelos compartimentales basados en ecuaciones diferenciales (se suele señalar a \cite{Kermack1927} como el principal antecedente) han sido adaptados a una gran diversidad de escenarios epidemiológicos permitiendo predicción de picos epidémicos, simulación de medidas de prevención y mitigación, estimación de parámetros, etc. Estos se basan en considerar una subpoblación dividida en compartimentos de acuerdo a su estado epidemiológico. Por ejemplo, un modelo sencillo de este tipo podría considerar a las subpoblaciones de susceptibles, infectados y recuperados. En general se utiliza una ecuación diferencial para representar la evolución temporal de cada uno de estos compartimentos.

Los ejemplos de aplicación de asimilación de datos sobre estos modelos epidemiológicos no son tan abundantes como en otras áreas. El trabajo de \cite{Ionides2006} es relevante en este aspecto puesto que utiliza filtros de partículas sobre un modelo de cólera. Además introduce una técnica de inferencia para sistemas no lineales que permite la estimación de parámetros del modelo. Por otro lado \cite{Shaman2012} usa el filtro de Kalman por ensambles acoplado a un modelo de gripe para predecir picos epidémicos en Nueva York. Luego, con el advenimiento de la pandemia de COVID-19 este tipo de abordaje se hizo algo más popular (por ejemplo \cite{Evensen2020, Li2020}). A pesar de que la asimilación de datos en estos sistemas comenzó a ser más común, la estimación de errores observacionales y de modelo en estos trabajos es infrecuente. Los modelos epidemiológicos suelen simplificar numerosas características dentro de la complejidad inherente a sistemas de este tipo, por lo que el error de modelo es inevitable. Por otro lado las observaciones sobre estos sistemas suelen constar de reportes de instituciones de salud y pueden incluir una diversidad de errores como subreportes, diagnósticos incorrectos, heterogeneidad en el monitoreo, etc. Esto motivó que tomemos un modelo epidemiológico compartimental sencillo para COVID-19 como ejemplo de aplicación de nuestro algoritmo EM \textit{online} con el objetivo de estudiar el efecto de estos errores en sistemas epidemiológicos y su respuesta al ser acoplados a métodos de inferencia de este tipo.

Los modelos epidemiológicos compartimentales están típicamente representados mediante ecuaciones diferenciales y las técnicas de asimilación de datos son en general aplicadas a modelos dinámicos con esta característica. Sin embargo, se han popularizado, en parte a las posibilidades que abre el aumento del poder de cómputo, los modelos basados en agentes. Estos, en lugar de tomar las variables de interés y representar su comportamiento mediante ecuaciones, simulan una población de individuos a través de un conjunto de reglas de interacción. El comportamiento global del sistema es entonces el resultado del comportamiento colectivo de la totalidad de los agentes. Este paradigma provee gran adaptabilidad y expresividad a estos modelos ya que permiten representar, mediante reglas simples de interrelación entre individuos, situaciones que pueden ser muy difíciles de reproducir mediante ecuaciones diferenciales. Los modelos basados en agentes pueden constituir una representación de alguna manera más directa para sistemas sociales. Es común que estos modelos requieran de una gran cantidad de parámetros debido a la cantidad de detalles que se deben tener en cuenta. Esto, sumado a la creciente popularidad y complejización de este tipo de modelos, ha causado que cobre relevancia el problema de estimar los parámetros para lograr una calibración adecuada. Además, para mejorar las capacidades de los modelos basados en agentes, es importante realizar tareas de inferencia estadística de manera de utilizar datos observacionales de los fenómenos que se modelan y así mejorar calidad de la representación. Esto permitiría obtener evoluciones temporales de la población de individuos acopladas a las restricciones estadisticas de los datos.

La asimilación de datos ha encontrado en el modelado epidemiológico más tradicional un campo de aplicación con resultados favorables y por lo tanto resulta prometedora la idea de extender su aplicabilidad a modelos basados en agentes. Con la notable excepción del trabajo de \cite{Ward2016}, en el que trabaja con un modelo de movimiento de peatones, no existen en nuestro conocimiento ejemplos de la utilización de técnicas de asimilación de datos en modelos de agentes. Esto nos motivó a seguir esta línea de investigación para lo cual desarrollamos una metodología general para utilizar técnicas de asimilación por ensambles con modelos de agentes. Como aplicación diseñamos un modelo de COVID-19 para el cual evaluamos experimentalmente el método. El diseño de los experimentos busca dar cuenta de algunas de las dificultades comunes en la inferencia de sistemas de propagación de COVID-19, como la estimación de la cantidad de asintomáticos, así como desafíos típicos de asimilación de datos tales como dar cuenta del error proveniente de un modelo mal especificado.

El surgimiento del COVID-19 dio lugar a que, entendiblemente, el interés de la comunidad científica se concentre en esta enfermedad y que proliferen modelos y métodos de inferencia para intentar comprender mejor este fenómeno, intentar dar predicciones y asesorar instituciones de salud y a tomadores de decisiones con el propósito de minimizar los daños de la crisis sanitaria global que significó la pandemia. Debido a esto y también por la gran disponibilidad de datos accesibles por el monitoreo de contagios, decidimos usar al COVID-19 como aplicación principal de los métodos desarrollados en la tesis.

% Al momento de iniciar el trabajo de este doctorado planificábamos trabajar con modelos de Dengue. Esta a enfermedad tiene la particularidad de que la transmisión no es persona a persona sino que es transmitida por vectores (en este caso mosquitos) y esto da lugar a modelos compartimentales interesantes con infecciones cruzadas como el de \cite{Kenkre2005}. La motivación por estudiar la propagación del Dengue tiene que ver con el problema sanitario que significa en Argentina y continuaba el trabajo iniciado en mi tesis de Licenciatura en Matemática \citep{Cocucci2015} en el que utilicé un modelo para el West Nile Virus, una enfermedad transmitida por mosquitos. Sin embargo el surgimiento del COVID-19 dio lugar a que entendiblemente, el interés de la comunidad científica se concentre en esta enfermedad y que proliferen modelos y métodos de inferencia para intentar comprender mejor este fenómeno. Debido a esto y también por la gran disponibilidad de datos accesibles por el monitoreo de la pandemia, decidimos usar al COVID-19 como ejemplo de aplicación.

En el Capítulo \ref{chp:da} formulamos el problema de la asimilación de datos desde una perspectiva Bayesiana e introducimos las técnicas más relevantes para el desarrollo de nuestro trabajo. En el Capítulo \ref{chp:error_treatment} introducimos el problema de tratamiento de errores, discutimos algunas de las metodologías más conocidas y finalmente presentamos nuestro algoritmo EM \textit{online} con su deducción teórica y una evaluación experimental de su desempeño. En el Capítulo \ref{chp:epi_models} introducimos los elementos básicos del modelado epidemiológico y los antecedentes de inferencia estadística basada en asimilación de datos en este tipo de escenario. Luego mostramos los resultados de la aplicación del EM \textit{online} en un modelo epidemiológico compartimental. En el Capítulo \ref{chp:da_abms} damos un marco general de aplicación de asimilación de datos basada en ensambles para modelos basados en agentes y hacemos una evaluación experimental de la metodología sobre un modelo que desarrollamos en base a las características epidemiológicas del COVID-19. Finalmente, damos nuestras conclusiones y discutimos potenciales líneas de investigación que se desprenden del trabajo en el Capítulo \ref{chp:conclusion}.
