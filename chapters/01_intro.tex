\chapter{Introducción}
En esta tesis exploramos algunos de los desafíos asociados a la aplicación de técnicas de asimilación de datos basadas en ensambles sobre modelos epidemiológicos. Abordamos el problema de la especificación de la incerteza inherente al modelo y las observaciones tanto en un marco general de modelos parcialmente observados como para el caso específico de modelos epidemiológicos basados en ecuaciones diferenciales. Además estudiamos el potencial de utilizar técnicas de asimilación de datos en modelos basados en agentes.

La asimilación de datos comprende un conjunto de técnicas estadísticas que se utilizan para combinar dos fuentes de información distintas sobre el estado de un mismo sistema: pronósticos provenientes de modelos matemáticos y observaciones. La disciplina está fuertemente emparentada con la predicción numérica meteorológica pues gran parte de su desarrollo está orientada a esos fines \citep{Talagrand1987}. En esta área, se cuenta con modelos matemáticos y computacionales muy complejos y de alta dimensionalidad que informan sobre diversas variables de estado (por ejemplo, velocidad, temperatura o presión) en diferentes puntos de una grilla espacial potencialmente muy grande ($10^7 - 10^8$ dimensiones). Estos modelos se basan en leyes físicas (por ejemplo, las ecuaciones de Navier-Stokes que expresan la conservación del momento y de la masa en fluidos), y permiten obtener pronósticos. Por otro lado, se tiene otra fuente de información sobre el mismo sistema que consta de las observaciones de diversos instrumentos en estaciones meteorológicas o provenientes de satélites. Ambas fuentes de información son propensas a errores. El error de modelo que comprende nuestro conocimiento limitado de la dinámica del sistema, aproximaciones y errores numéricos. El error observacional incuye la incerteza propia de los instrumentos de medición y el error de representatividad que involucra como se relacionan las observaciones con el estado del sistema (más sobre esto en la Sección \ref{sec:model_obs_error}). La asimilación de datos apunta a encontrar una combinación ponderada entre estas fuentes de información, de manera que si sabemos que la incerteza del modelo es menor que la de los datos, la estimación resultante será más fiel al modelo y si por el contrario, las observaciones tienen menos error que el pronóstico la estimación será más próxima a los datos.

El filtro de Kalman \citep{Kalman1960, Kalman1961} ocupa un lugar central dentro de las técnicas de asimilación de datos pues es una metodología sencilla que ha sentado las bases para métodos más sofisticados. Este tipo de filtro lineal encontró aplicaciones relevantes en la determinación de órbitas satelitales, navegación de submarinos y aeronaves e incluso de misiones espaciales como la Apollo \citep{Jazwinski1970}. En este tipo de aplicaciones típicamente tenemos que con el fin de estimar la posición y velocidad se utiliza como modelo a las ecuaciones físicas de movimiento mientras que las observaciones provienen de los instrumentos de navegación. Una gran parte del desarrollo de técnicas de asimilación de datos proviene sin embargo del área de las geociencias donde se presentan otro tipo de desafíos como la alta dimensionalidad de los sistemas, observaciones menos precisas y modelos caóticos altamente no-lineales. Por ejemplo, el filtro de Kalman por ensambles \citep{Evensen1994} toma la idea original de Kalman incorpora la idea de representar distribuciones mediante muestras lo cual permite adaptar el problema a situaciones de no-linealidad. La familia de métodos por ensambles pudo competir con los más tradicionales métodos variacionales (3D-VAR y 4D-VAR) que formulan al problema como la minimización de una función de costo y que son utilizados en grandes centros meteorológicos \citep{Kalnay2007}. Otras técnicas más modernas han comenzado a ganar relevancia en el campo: notablemente los filtros de partículas que permiten la representación de distribuciones no Gaussianas han conformado otra gran familia de metodologías de creciente interés \citep{vanLeeuwen2019}. Muchas de las técnicas de asimilación de datos fueron desarrolladas inicialmente para resolver problemas asociados a la predicción numérica meteorológica pero el campo de apicación es mucho más diverso. Se utilizan, por ejemplo, para navegación aeorespacial \cite{Grewal2010}, predicción sobre reservorios petrolíferos \cite{Aanonsen2009}, oceanografía, detección de incendios forestales \citep{Mandel2008}, epidemiología \citep{Shaman2012}, entre otras. En el Capítulo \ref{chp:da} formulamos el problema de la asimilación de datos desde una perspectiva Bayesiana e introducimos las técnicas más relevantes para el desarrollo de nuestro trabajo. En \cite{Carrassi2018} se puede encontrar un buen panorama del estado del arte en cuanto a metodologías de asimilación de datos.

Actualmente el creciente interés en machine learning y los grandes avances en el área en las últimas décadas motivó interés en el intercambio entre esta área y la asimilación de datos. Por ejemplo, en \cite{Abarbanel2018} se plantea una interpretación de las redes neuronales en el contexto de  problemas de asimilación de datos dando una equivalencia entre agregar capas a la red neuronal con la resolución temporal en el problema de asimilación. Además también se señala la utilidad de algunas herramientas de asimilación para su aplicación en machine learning. En esta dirección \cite{Kovachki2019} propone el uso técnicas basadas en filtros de Kalman por ensambles para asistir el entrenamiento de modelos supervisados y semisupervisados de manera que se evita el uso de gradientes.

Como mencionamos anteriormente, la asimilación de datos tiene en cuenta la incerteza del modelo que genera los pronósticos del sistema tanto como la proveniente de las observaciones. Una especificación errónea de estas cantidades puede causar una performance subóptima de la inferencia pero es habitual que estos errores sean difíciles de identificar. Por lo tanto, existe una variedad de métodos para proveer estimaciones para el error observacional y de modelo \citep{Tandeo2020}. Estos incluyen metodologías basadas en momentos estadísticos y otras que apuntan a la maximización de la verosimilitud entre las cuales hacemos mención del algoritmo EM \citep{Dempster1977}, el cual puede ser implementado en el contexto de asimilación de datos con filtros de Kalman y en particular con filtros basados en ensambles \citep{Tandeo2015}. La implementación clásica del algoritmo EM toma un lote (\textit{batch}) de datos y los procesa a todos juntos para dar proveer estimaciones de los parámetros que codifican los errores. Esto trae una serie de inconvenientes a la hora de ser utilizado en sistemas de asimilación secuencial en los que los datos son producidos y procesados en tiempo real o casi real. El procesamiento por lotes (\textit{offline}) por un lado, fuerza a almacenar todo el conjunto de observaciones y por otro, cuando ingresa una observación no es en principio adaptable a incorporarla por lo que el proceso debe reiniciarse. Esta situación ha despertado interés en técnicas de inferencia \textit{online}, es decir que permitan actualizaciones con cada nueva observación y de manera que cada dato es procesado una única vez. En esta dirección se destaca la metodología desarrollada en \cite{Berry2013} la cual es basada en innovaciones (esto, a grandes rasgos significa que se basa en las diferencias entre pronósticos y observaciones) y que está específicamente diseñada para contextos de asimilación de datos. Por otro lado, luego del trabajo seminal de \cite{Neal1998} tenemos versiones \textit{online} del EM por \cite{Andrieu2003} y por Cappé \citep{Cappe2009,Cappe2011}.Estas sin embargo no están explícitamente diseñadas para ser acopladas a técnicas clásicas de asimilación de datos. Por el interés en metodologías de este tipo y su relevancia nos vimos motivados a desarrollar una versión \textit{online} del EM que se adapta a estrategias de asimilación de datos por ensambles. En el Capítulo \ref{chp:error_treatment} introducimos el problema de tratamiento de errores, discutimos algunas de las metodologías más conocidas y finalmente presentamos nuestro algoritmo EM \textit{online} con su deducción teórica y una evaluación experimental de su desempeño.

El marco en el que se suele aplicar la asimilación de datos es el de un sistema que evoluciona temporalmente y es parcialmente observado. A pesar de no ser una de las áreas típicas de aplicación, los sistemas epidemiológicos tienen estas características y pueden servise de las herramientas de la asimilación de datos. Desde comienzos del siglo pasado se han utilizado modelos matemáticos para estudiar las dinámicas de transmisión de enfermedades. Los populares modelos compartimentales basados en ecuaciones diferenciales \citep{Kermack1927} han sido adaptados a una gran diversidad de escenarios epidemiológicos permitiendo predicción de picos epidémicos, simulación de medidas de prevención y mitigación, estimación de parámetros, etc. Sin embargo, los ejemplos de aplicación de asimilación de datos sobre estos modelos no son tan abundantes como en otras áreas. Los trabajos de \cite{Ionides2006} y \cite{Shaman2012} marcaron un precedente en este sentido: en el primero se utilizan filtros de partículas y en el segundo filtro de Kalman por ensambles para un modelo de cólera y gripe respectivamente. Luego, con el advenimiento de la pandemia de COVID-19 este abordaje se hizo algo más popular (por ejemplo \cite{Evensen2020, Li2020}). A pesar de que la asimilación de dtos en estos sistemas comenzó a ser más popular, la estimación de errores observacionales y de modelo en estos trabajos es infrecuente. Esto motivó que  tomemos un modelo epidemiológico compartimental sencillo para COVID-19 como ejemplo de aplicación de nuestro algoritmo EM \textit{online} y con la motivación de estudiar el efecto de estos errores en sistemas epidemiológicos. En el Capítulo \ref{chp:epi_models} mostramos los resultados previamente introduciendo los elementos básicos del modelado epidemiológico y los antecedentes de inferencia estadística basada en asimilación de datos en este tipo de escenario.

Los modelos epidemiológicos compartimentales están típicamente representados mediante ecuaciones diferenciales y las técnicas de asimilación de datos son en general aplicadas a modelos dinánmicos con esta característica. Sin embargo, se han popularizado, en parte a las posiblidades que abre el aumento del poder de cómputo, los modelos basados en agentes. Estos, en lugar de tomar las variables de interés y representar su comportamiento mediante ecuaciones, simulan una población de individuos a través de un conjunto de reglas de interacción. El comportamiento global del sistema es entonces el resultado del comportamiento colectivo de la totalidad de los agentes. Este paradigma provee gran adaptabilidad y expresividad a estos modelos ya que permiten representar situaciones mediante reglas simples de interrelación entre individuos que pueden ser muy difíciles de reproducir mediante ecuacuiones diferenciales. Con la notable excepción del trabajo de \cite{Ward2016}, no existen en nuestro conocimiento aplicaciones de asimilación de datos sobre modelos de agentes. En el Capítulo \ref{chp:da_abms} damos un marco general de aplicación de asimilación de datos basada en ensambles para modelos basados en agentes y hacemos una evaluación experimental de la metodología sobre un modelo que desarrollamos en base a las características epidemiológicas del COVID-19.

Al momento de inicial el trabajo de este doctorado planificabamos trabajar con modelos de Dengue. Esta a enfermedad tiene la particularidad de que la transmisión no es persona a persona sino que es transmitida por vectores (en este caso mosquitos) y esto da lugar a modelos compartimentales interesantes como por ejemplo \cite{}. La motivación por estudiar la propagación del Dengue tiene que ver con el problema sanitario que significa en Argentina y continuaba el trabajo iniciado en mi tesis de Licenciatura en Matemática \citep{}. Sin embargo el surgimiento del COVID-19 dio lugar a que entendiblemente, el interés de la comunidad científica se concentre en esta enfermedad y que proliferen modelos y métodos de inferencia para intentar comprender mejor este fenómeno. Debido a esto y también por la gran disponibilidad de datos accesibles por el monitoreo de la pandemia, decidimos usar al COVID-19 como ejemplo de aplicación
